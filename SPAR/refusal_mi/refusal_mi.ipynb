{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install and import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install transformers torch einops transformer_lens plotly circuitsvis numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import einops\n",
    "import transformer_lens\n",
    "import functools\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformer_lens import ActivationCache\n",
    "from transformer_lens import utils as tl_utils\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from torch import Tensor\n",
    "from jaxtyping import Int, Float\n",
    "from typing import Tuple, List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hugging Face credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a `.env` file at the root of this repo with the following format (see `.env.example`):\n",
    "```\n",
    "HF_USERNAME=bob\n",
    "HF_TOKEN=token123\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read HuggingFace credentials from .env file\n",
    "with open('../../.env', 'r') as file:\n",
    "  for line in file:\n",
    "    key, value = line.strip().split('=', 1)\n",
    "    os.environ[key] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name_path = \"meta-llama/Llama-2-7b-chat-hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_path,\n",
    "    use_auth_token=os.environ[\"HF_TOKEN\"],\n",
    "    low_cpu_mem_usage=True,\n",
    "    use_cache=False,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name_path,\n",
    "    use_auth_token=os.environ[\"HF_TOKEN\"],\n",
    "    use_fast=False\n",
    ")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "tokenizer.padding_side = 'left'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens import HookedTransformer, HookedTransformerConfig\n",
    "\n",
    "tl_model = HookedTransformer.from_pretrained(\n",
    "    model_name_path,\n",
    "    hf_model=model,\n",
    "    device='cpu',\n",
    "    fold_ln=False,\n",
    "    center_writing_weights=False,\n",
    "    center_unembed=False,\n",
    "    tokenizer=tokenizer\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define utils to help with prompting and generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def instruction_to_prompt(instruction, system_prompt=\"\", model_output=\"\") -> str:\n",
    "    \"\"\"\n",
    "    Converts an instruction to a prompt string structured for Llama2-chat.\n",
    "    Note that, unless model_output is supplied, the prompt will (intentionally) end with a space.\n",
    "    See details of Llama2-chat prompt structure here: here https://huggingface.co/blog/llama2#how-to-prompt-llama-2\n",
    "    \"\"\"\n",
    "\n",
    "    B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "    B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "\n",
    "    if len(system_prompt) == 0:\n",
    "        dialog_content = instruction.strip()\n",
    "    else:\n",
    "        dialog_content = B_SYS + system_prompt.strip() + E_SYS + instruction.strip()\n",
    "    prompt = f\"{B_INST} {dialog_content}{E_INST} {model_output.strip()}\"\n",
    "    return prompt\n",
    "\n",
    "def instruction_to_prompt_toks(tokenizer, instruction, system_prompt=\"\", model_output=\"\") -> Int[Tensor, \"seq_len\"]:\n",
    "\n",
    "    prompt_toks = tokenizer.encode(\n",
    "        instruction_to_prompt(instruction, system_prompt, model_output)\n",
    "    )\n",
    "    return prompt_toks\n",
    "\n",
    "def tokenize_and_batch(instructions, system_prompt=\"\", model_outputs=None, batch_size=16):\n",
    "\n",
    "    if model_outputs:\n",
    "        assert(len(instructions) == len(model_outputs))\n",
    "        prompts = [instruction_to_prompt(instruction, system_prompt, model_output) for (instruction, model_output) in zip(instructions, model_outputs)]\n",
    "    else:\n",
    "        prompts = [instruction_to_prompt(instruction, system_prompt) for instruction in instructions]\n",
    "\n",
    "    num_batches = len(instructions) // batch_size + int(len(instructions) % batch_size != 0)\n",
    "    batches = []\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min(len(prompts), start_idx + batch_size)\n",
    "\n",
    "        batches.append(\n",
    "            tokenizer(\n",
    "                prompts[start_idx:end_idx],\n",
    "                padding=True,\n",
    "                truncation=False,\n",
    "                return_tensors=\"pt\"\n",
    "            ).input_ids\n",
    "        )\n",
    "\n",
    "    return batches\n",
    "\n",
    "def generate_from_instructions(instructions, max_new_tokens=64, temperature=0.0, batch_size=16):\n",
    "\n",
    "    batches = tokenize_and_batch(instructions, batch_size=batch_size)\n",
    "\n",
    "    for batch_idx, batch in enumerate(batches):\n",
    "        output_ids = tl_model.generate(batch.to(device), max_new_tokens=max_new_tokens, temperature=temperature)\n",
    "        for answer_idx, answer in enumerate(tokenizer.batch_decode(output_ids)):\n",
    "            print(f\"\\nGeneration #{batch_idx * batch_size + answer_idx}:\\n{answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_from_instructions(\n",
    "    [\n",
    "        \"Tell me how to make a pie.\",\n",
    "        \"Tell me how to make a really big pie.\",\n",
    "        \"Tell me how to make a bomb.\",\n",
    "        \"Tell me how to make a really big bomb.\",\n",
    "    ],\n",
    "    batch_size=2,\n",
    "    max_new_tokens=128\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation patching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation patching - text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harmful_prompt_toks  = tokenize_and_batch([\"Tell me how to make a bomb.\"])\n",
    "harmless_prompt_toks = tokenize_and_batch([\"Tell me how to make a pie.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harmful_logits, harmful_cache = tl_model.run_with_cache(harmful_prompt_toks[0])\n",
    "harmless_logits, harmless_cache = tl_model.run_with_cache(harmless_prompt_toks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bomb_tok_pos = tl_model.to_str_tokens(harmful_prompt_toks[0][0]).index(\"bomb\")\n",
    "pie_tok_pos  = tl_model.to_str_tokens(harmless_prompt_toks[0][0]).index(\"pie\")\n",
    "assert(bomb_tok_pos == pie_tok_pos)\n",
    "\n",
    "object_tok_pos = bomb_tok_pos\n",
    "print(f\"Position of object token (e.g. 'bomb' or 'pie') is: {object_tok_pos}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_patching_hook(\n",
    "    activation: Float[Tensor, \"batch seq d_activation\"],\n",
    "    hook: HookPoint,\n",
    "    pos: int,\n",
    "    cache_to_patch_from: ActivationCache,\n",
    ") -> Float[Tensor, \"batch seq d_activation\"]:\n",
    "\n",
    "    activation[:, pos, :] = cache_to_patch_from[hook.name][0, pos, :]\n",
    "    return activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_hooks(toks: Int[Tensor, \"batch_size seq_len\"], max_tokens_generated=64, fwd_hooks=[], include_prompt=False) -> str:\n",
    "    assert toks.shape[0] == 1, \"batch size must be 1\"\n",
    "    all_toks = torch.zeros((toks.shape[0], toks.shape[1] + max_tokens_generated), dtype=torch.long).to(device)\n",
    "    all_toks[:, :toks.shape[1]] = toks\n",
    "\n",
    "    for i in range(max_tokens_generated):\n",
    "        logits = tl_model.run_with_hooks(\n",
    "            all_toks[:, :-max_tokens_generated + i],\n",
    "            return_type=\"logits\",\n",
    "            fwd_hooks=fwd_hooks,\n",
    "        )[0,-1] # get the first element in the batch, and the last logits tensor in the sequence\n",
    "\n",
    "        # greedy sampling (temperature=0)\n",
    "        next_token = logits.argmax()\n",
    "        all_toks[0,-max_tokens_generated+i] = next_token\n",
    "\n",
    "    if include_prompt:\n",
    "        return tokenizer.decode(all_toks[0])\n",
    "    else:\n",
    "        return tokenizer.decode(all_toks[0, toks.shape[1]:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Patching harmful→harmless (e.g. bomb→pie)\\n\")\n",
    "\n",
    "for layer in range(tl_model.cfg.n_layers):\n",
    "\n",
    "    print(f\"\\nLAYER={layer}, POS={object_tok_pos}\")\n",
    "\n",
    "    hook_fn = functools.partial(\n",
    "        activation_patching_hook,\n",
    "        pos=object_tok_pos,\n",
    "        cache_to_patch_from=harmless_cache,\n",
    "    )\n",
    "\n",
    "    activation_patching_generation = generate_with_hooks(\n",
    "        harmful_prompt_toks[0],\n",
    "        max_tokens_generated=64,\n",
    "        fwd_hooks=[(tl_utils.get_act_name(\"resid_post\", layer), hook_fn)],\n",
    "        include_prompt=False\n",
    "    )\n",
    "\n",
    "    print(repr(activation_patching_generation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "- Layers 0-5: **answer** about how to make a **pie**\n",
    "- Layer 6-14: **refusal** about how to make a **pie**\n",
    "- Layer 15-31: **refusal** about how to make a **bomb**\n",
    "\n",
    "It seems like a change from answer→refusal takes place around layer 6, and a change from pie→bomb takes place around layer 15."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation patching - top logits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "from torch import Tensor\n",
    "\n",
    "def plot_topk_tokens(logits: Float[Tensor, \"d_vocab\"], tokenizer: AutoTokenizer, topk=5, title_suffix=\"\"):\n",
    "    assert (logits.ndim == 1), \"only supports logits.ndim=1\"\n",
    "\n",
    "    topk_values, topk_ids = logits.topk(topk)\n",
    "    topk_toks = [tokenizer.decode([tok.item()]) for tok in topk_ids]\n",
    "\n",
    "    fig = go.Figure(\n",
    "        go.Bar(\n",
    "            y=topk_toks,\n",
    "            x=topk_values.cpu().numpy(),\n",
    "            orientation='h',\n",
    "            marker=dict(color='skyblue')\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title_text=f'Top-{topk} logits {title_suffix}',\n",
    "        xaxis_title='Value',\n",
    "        yaxis_title='Token',\n",
    "        height=50*topk, width=450,\n",
    "        showlegend=False\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tok_set = set()\n",
    "\n",
    "for layer in range(32):\n",
    "    print(f\"\\nLAYER={layer}, POS={object_tok_pos}\")\n",
    "\n",
    "    hook_fn = functools.partial(\n",
    "        activation_patching_hook,\n",
    "        pos=object_tok_pos,\n",
    "        cache_to_patch_from=harmless_cache,\n",
    "    )\n",
    "\n",
    "    activation_patching_logits = tl_model.run_with_hooks(\n",
    "        harmful_prompt_toks[0],\n",
    "        fwd_hooks=[(tl_utils.get_act_name(\"resid_post\", layer), hook_fn)],\n",
    "    )\n",
    "\n",
    "    last_pos_logits = activation_patching_logits[0, -1, :]\n",
    "\n",
    "    topk_values, topk_ids = last_pos_logits.topk(k=8)\n",
    "    topk_toks = [tokenizer.decode([tok.item()]) for tok in topk_ids]\n",
    "\n",
    "    print(f\"topk_toks: {topk_toks}\")\n",
    "\n",
    "    for tok_id in topk_ids:\n",
    "        pred_tok_set.add(tok_id.item())\n",
    "\n",
    "    # plot_topk_tokens(\n",
    "    #     last_pos_logits,\n",
    "    #     tokenizer,\n",
    "    #     topk=5,\n",
    "    #     title_suffix=f\"Patching bomb→pie at layer={layer}\"\n",
    "    # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tok_list = list(pred_tok_set)\n",
    "pred_tok_str_list = [tokenizer.decode([tok], clean_up_tokenization_spaces=False) for tok in pred_tok_list]\n",
    "\n",
    "print(f\"Strings appearing in at least one topk result:\\n{pred_tok_str_list}\")\n",
    "print(pred_tok_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tok_logits = np.zeros((len(pred_tok_set), tl_model.cfg.n_layers))\n",
    "\n",
    "for layer in range(32):\n",
    "\n",
    "    hook_fn = functools.partial(\n",
    "        activation_patching_hook,\n",
    "        pos=object_tok_pos,\n",
    "        cache_to_patch_from=harmless_cache,\n",
    "    )\n",
    "\n",
    "    activation_patching_logits = tl_model.run_with_hooks(\n",
    "        harmful_prompt_toks[0],\n",
    "        fwd_hooks=[(tl_utils.get_act_name(\"resid_post\", layer), hook_fn)],\n",
    "    )\n",
    "\n",
    "    for i, top_tok in enumerate(pred_tok_list):\n",
    "        pred_tok_logits[i, layer] = activation_patching_logits[0, -1, top_tok].item()\n",
    "\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "for i, tok in enumerate(pred_tok_list):\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=list(range(tl_model.cfg.n_layers)), \n",
    "            y=pred_tok_logits[i],\n",
    "            mode='lines+markers',\n",
    "            name=f'{pred_tok_str_list[i]}'\n",
    "        )\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Logits for common tokens vs patching layer',\n",
    "    xaxis=dict(title='Patching layer'),\n",
    "    yaxis=dict(title='Logit value')\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A token with good signal will have very different logit values for early patching (close to the `pie` input) and later patching (close to the `bomb` input)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tok_logit_difs = pred_tok_logits[:, -1] - pred_tok_logits[:, 0]\n",
    "\n",
    "print(f\"\\nToks with positive dif (reflective of refusal)\")\n",
    "refusal_tok_difs, refusal_tok_idxs = torch.tensor(pred_tok_logit_difs).topk(5)\n",
    "for i in refusal_tok_idxs:\n",
    "    print(f\"{pred_tok_str_list[i]} ({pred_tok_list[i]}): {pred_tok_logit_difs[i]:+0.2f}\")\n",
    "\n",
    "print(f\"\\nToks with negative dif (reflective of answer)\")\n",
    "answer_tok_difs, answer_tok_idxs = torch.tensor(-pred_tok_logit_difs).topk(5)\n",
    "for i in answer_tok_idxs:\n",
    "    print(f\"{pred_tok_str_list[i]} ({pred_tok_list[i]}): {pred_tok_logit_difs[i]:+0.2f}\")\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "for i, tok in enumerate(pred_tok_list):\n",
    "    if i not in refusal_tok_idxs and i not in answer_tok_idxs:\n",
    "        continue\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=list(range(tl_model.cfg.n_layers)), \n",
    "            y=pred_tok_logits[i],\n",
    "            mode='lines+markers',\n",
    "            name=f'{pred_tok_str_list[i]}'\n",
    "        )\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Logits for common tokens vs patching layer, polarizing tokens only',\n",
    "    xaxis=dict(title='Patching layer'),\n",
    "    yaxis=dict(title='Logit value')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
